{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f28b224a-ccb1-46e6-a552-241f4c009b98",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation) with Agentic AI Demo\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a RAG (Retrieval-Augmented Generation) system using LlamaStack, which combines:\n",
    "- **Document Retrieval**: Using vector databases to search through ingested documents\n",
    "- **Agentic AI**: Using ReAct (Reasoning + Acting) agents that can use multiple tools\n",
    "- **Multi-Tool Workflows**: Combining RAG, web search, and custom tools for comprehensive question answering\n",
    "\n",
    "## Approach & Architecture\n",
    "\n",
    "### Why RAG?\n",
    "RAG addresses the limitation of LLMs having static knowledge by:\n",
    "1. **Retrieval**: Finding relevant information from a knowledge base (vector database)\n",
    "2. **Augmentation**: Adding retrieved context to the prompt\n",
    "3. **Generation**: Using the LLM to generate answers based on the augmented context\n",
    "\n",
    "### Why Agentic AI?\n",
    "Traditional RAG only searches documents. Agentic AI enables:\n",
    "- **Tool Selection**: Automatically choosing the right tool (RAG, web search, custom tools)\n",
    "- **Multi-Step Reasoning**: Breaking down complex queries into steps\n",
    "- **Dynamic Information**: Accessing real-time data (stock prices, web search)\n",
    "\n",
    "### System Components\n",
    "1. **LlamaStack Client**: Single point interface to LLM services, vector databases and agents\n",
    "2. **Vector Database (Milvus)**: Stores document embeddings for semantic search\n",
    "3. **Docling**: Advanced PDF extraction with OCR capabilities\n",
    "4. **ReAct Agents**: Intelligent agents that reason and act using tools\n",
    "5. **Custom Tools**: Domain-specific functions (e.g., Yahoo Finance for stock data)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3515cea4-91ff-467d-a8e1-03271dfe9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install notebook dependencies. \n",
    "# Will take a while to download and install numerous dependencies. \n",
    "# Wait until it finishes before proceeding\n",
    "%pip install llama_stack_client==0.2.22 docling yfinance rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087b83d-dd81-4f1e-b634-47ce809fe33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the installed version of llama_stack_client (0.2.22)\n",
    "# This ensures we're using the correct version for compatibility\n",
    "import llama_stack_client\n",
    "\n",
    "print(llama_stack_client.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b81f35-2028-4d66-8200-117e57132c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python stdlib imports\n",
    "import os\n",
    "import json\n",
    "from datetime import date, datetime, timedelta\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Suppress verbose and noisy HTTP logs\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "# Llamastack imports\n",
    "from llama_stack_client import LlamaStackClient, Agent, AgentEventLogger\n",
    "from llama_stack_client import Document\n",
    "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
    "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput\n",
    "from llama_stack_client.lib.agents.client_tool import client_tool\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "\n",
    "# Yahoo finance API for custom agents\n",
    "import yfinance as yf\n",
    "\n",
    "# Docling imports\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# pretty printing\n",
    "import rich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770b2567-d67a-4ced-a8aa-d1c27f8b9faf",
   "metadata": {},
   "source": [
    "## Setting up Configurations\n",
    "\n",
    "### LLM Sampling Parameters\n",
    "\n",
    "These parameters control how the LLM generates responses:\n",
    "\n",
    "- **temperature**: Controls randomness (0.0 = deterministic, 1.0+ = more creative)\n",
    "  - Lower values (0.1-0.3): More focused, deterministic responses\n",
    "  - Higher values (0.7-1.0): More creative, diverse responses\n",
    "  - We use 0.7 for balanced creativity and accuracy\n",
    "\n",
    "- **top_p** (nucleus sampling): Probability mass threshold for token selection\n",
    "  - Only considers tokens whose cumulative probability is within top_p\n",
    "  - 0.95 means considering tokens that make up 95% of probability mass\n",
    "  - Works with temperature to control diversity\n",
    "\n",
    "- **max_tokens**: Maximum number of tokens in the generated response\n",
    "  - Prevents excessively long outputs\n",
    "  - 512 tokens ≈ 400-500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0abe1e-7876-417c-890f-4f026cff508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature: Controls randomness in LLM output\n",
    "# 0.0 = deterministic (always same output for same input)\n",
    "# 0.7 = balanced creativity and consistency\n",
    "# 1.0+ = highly creative/variable outputs\n",
    "temperature = 0.7\n",
    "\n",
    "# Configure sampling strategy based on temperature\n",
    "if temperature > 0.0:\n",
    "    # Top-p (nucleus sampling): Only consider tokens whose cumulative probability \n",
    "    # is within the top_p threshold (0.95 = 95% probability mass)\n",
    "    # This provides more focused sampling than pure temperature\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    # Top-p strategy: Uses both temperature and top_p for controlled randomness\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    # Greedy strategy: Always selects the most probable token (deterministic)\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "# Maximum tokens in the generated response\n",
    "# 512 tokens ≈ 400-500 words, prevents excessively long outputs\n",
    "max_tokens = 512\n",
    "\n",
    "# Sampling parameters dictionary\n",
    "# Will be passed to LlamaStack Agents/Inference APIs to control text generation\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3995740-dfa4-49fa-8d50-6d0d29669eb7",
   "metadata": {},
   "source": [
    "## Initializing LlamaStack Client and Selecting Models\n",
    "\n",
    "### Client Setup\n",
    "The LlamaStackClient connects to the LlamaStack service endpoint, which provides:\n",
    "- LLM inference services\n",
    "- Vector database management\n",
    "- Agent orchestration\n",
    "- Tool execution\n",
    "\n",
    "### Model Selection\n",
    "We need two types of models:\n",
    " 1. **LLM Model**: For text generation (e.g., Granite-3.3-8B-Instruct)\n",
    " 2. **Embedding Model**: For converting text to vectors (e.g., granite-embedding-125m)\n",
    "    - **Embedding Dimension**: Size of the vector space (e.g., 768 dimensions)\n",
    "    - Used for semantic similarity search in vector databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a5780f-fc9a-48a2-baad-8db0817b19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaStack service URL (in-cluster)\n",
    "LLAMASTACK_URL = \"http://llama-stack-dist-service.competitor-analysis.svc.cluster.local:8321\"\n",
    "\n",
    "# Vector DB name (logical identifier used by Llamastack)\n",
    "VECTOR_DB_NAME = \"agentic-rag-db\"\n",
    "\n",
    "# Initialize client\n",
    "client = LlamaStackClient(base_url=LLAMASTACK_URL)\n",
    "    \n",
    "# Test connection by listing models\n",
    "models = client.models.list()\n",
    "    \n",
    "rich.print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2e45d-7bb4-47b2-9a8c-3e4c7385e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the main inference model and embedding model\n",
    "model_id = next(m.identifier for m in models if m.model_type == \"llm\")\n",
    "embedding_model = next(m for m in models if m.model_type == \"embedding\")\n",
    "embedding_model_id = embedding_model.identifier\n",
    "embedding_dimension = int(embedding_model.metadata[\"embedding_dimension\"])\n",
    "\n",
    "vector_db = client.vector_dbs.register(\n",
    "    vector_db_id=VECTOR_DB_NAME,\n",
    "    embedding_model=embedding_model_id,\n",
    "    embedding_dimension=embedding_dimension,\n",
    "    provider_id=\"milvus-remote\",\n",
    ")\n",
    "\n",
    "# IMPORTANT: Need to use vector DB identifier UUID instead of logical name for ingestion and queries\n",
    "vector_db_id = vector_db.identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e3ddee-c502-4552-a891-ada89860be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(f\"Using inference model: {model_id}\")\n",
    "rich.print(f\"Using embedding model: [red]{embedding_model_id}[/red] with dimension: {embedding_dimension}\")\n",
    "rich.print(f\"Using vector DB with ID: [red]{vector_db_id}[/red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3456b6d9-f794-47e6-bf85-d54065f99100",
   "metadata": {},
   "source": [
    "## Document Ingestion using Docling\n",
    "\n",
    "### Why Docling?\n",
    "Docling is an advanced document converter that provides:\n",
    "- **Intelligent PDF Parsing**: Extracts text, tables, and structure\n",
    "- **OCR Capabilities**: Handles scanned documents and images\n",
    "- **Table Extraction**: Preserves table structure and formatting\n",
    "- **Better than Basic Extractors**: Maintains document hierarchy and context\n",
    "\n",
    "### Document Sources\n",
    "As an example, we will ingest Indian Bank financial documents from their official website:\n",
    "- Financial results\n",
    "- Presentations\n",
    "- Notes and disclosures\n",
    "\n",
    "> WARNING: This approach of listing URLs manually should only be used during development and testing!. For bulk ingestion of documents, use the KFP pipeline approach outlined in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d62bd-f668-41cd-be0c-3b55e8edbc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs of sample Indian Bank financial documents to ingest\n",
    "# These PDFs contain financial results, presentations, and notes\n",
    "urls = [\n",
    "     \"https://indianbank.bank.in/wp-content/uploads/2025/10/Notes-forming-part-of-Reviewed-Financial-Results-for-September-2025.pdf\",\n",
    "     \"https://indianbank.bank.in/wp-content/uploads/2025/10/Presentation-September-2025.pdf\",\n",
    "     \"https://indianbank.bank.in/wp-content/uploads/2025/10/Reviewed-Financial-Results-Consolidated.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c6f3f5-c707-4c14-9c30-d5c290f57495",
   "metadata": {},
   "source": [
    "## Docling-Powered Document Ingestion\n",
    "\n",
    "### Ingestion Process\n",
    "1. **Download & Convert**: Docling converts PDF's to structured text (Markdown)\n",
    "2. **Create Document Objects**: Wrap extracted text in Llamastack `Document` objects with metadata\n",
    "3. **Chunk & Embed**: Documents are split into chunks and converted to embeddings\n",
    "4. **Store in Vector DB**: Chunks are stored in the vector database for retrieval\n",
    "\n",
    "### Chunking Strategy\n",
    "- **chunk_size_in_tokens**: 512 tokens per chunk\n",
    "  - Balances context size with retrieval precision\n",
    "  - Smaller chunks = more precise matches\n",
    "  - Larger chunks = more context per match\n",
    "\n",
    "> WARNING: Converting documents using Docling will take a lot of time depending on your GPU and hardware capacity. Wait until the conversion is complete, and the embeddings ingested into the vector database before proceeding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa16fa-2c0e-49ee-92f6-21cda31c16da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through URLs and get the PDFs\n",
    "# Pass the PDFs to Docling for conversion\n",
    "# Will take a while depending on if you have GPUs\n",
    "# Wait until it finishes.\n",
    "\n",
    "for pdf_file in urls:\n",
    "    # Initialize docling converter\n",
    "    converter = DocumentConverter()\n",
    "    result = converter.convert(pdf_file)\n",
    "    text_content = result.document.export_to_markdown()\n",
    "\n",
    "    document_2 = Document(\n",
    "        document_id=f\"{pdf_file}\",\n",
    "        content=text_content,\n",
    "        mime_type=\"text/markdown\",\n",
    "        metadata={\"source\": pdf_file}\n",
    "    )\n",
    "\n",
    "    # Insert into vector DB\n",
    "    client.tool_runtime.rag_tool.insert(\n",
    "        documents=[document_2],\n",
    "        vector_db_id=vector_db_id, \n",
    "        chunk_size_in_tokens=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc95c8-6180-4c61-822d-ea2e87977962",
   "metadata": {},
   "source": [
    "### a. Manual RAG Search\n",
    "\n",
    "**Approach**: Direct control over retrieval and generation steps.\n",
    "\n",
    "**Process**:\n",
    "1. Query the vector database for relevant chunks\n",
    "2. Format retrieved chunks as context\n",
    "3. Build a prompt with query + context\n",
    "4. Call LLM to generate answer\n",
    "\n",
    "**Advantages**:\n",
    "- Full control over retrieval parameters\n",
    "- Customizable prompt templates\n",
    "- Easy to debug and inspect intermediate steps\n",
    "\n",
    "**Use Cases**: When you need fine-grained control over the RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411706b8-c964-42c2-ae79-d4e3272ef901",
   "metadata": {},
   "source": [
    "#### Step 1: Retrieving Relevant Chunks\n",
    "\n",
    "**Query Configuration Parameters**:\n",
    "- **query_generator_config**: How to process the query\n",
    "  - `type: \"default\"`: Standard query processing\n",
    "  - `separator: \" \"`: Token separator for query parsing\n",
    "- **max_tokens_in_context**: Maximum total tokens from retrieved chunks (4096)\n",
    "- **max_chunks**: Number of chunks to retrieve (5)\n",
    "- **chunk_template**: Format for each chunk in the response\n",
    "- **mode: \"vector\"**: Use vector similarity search (semantic search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e9d3d-da1c-4232-850d-86b86186ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User query about Indian Bank shareholding\n",
    "query = \"As per the documents, tell me about Percentage of shares held by Government of India in Indian bank \"\n",
    "\n",
    "# Query the vector database for relevant document chunks\n",
    "# This performs semantic search to find chunks most similar to the query\n",
    "response = client.tool_runtime.rag_tool.query(\n",
    "        vector_db_ids=[vector_db_id],  # Which vector database(s) to search\n",
    "        content=query,  # The user's query/question\n",
    "        query_config={\n",
    "            # Query generation configuration\n",
    "            \"query_generator_config\": {\n",
    "                \"type\": \"default\",  # Standard query processing\n",
    "                \"separator\": \" \"  # Token separator for query parsing\n",
    "            },\n",
    "            # Maximum total tokens from all retrieved chunks\n",
    "            # Prevents exceeding LLM context window limits\n",
    "            \"max_tokens_in_context\": 4096,\n",
    "            # Maximum number of chunks to retrieve\n",
    "            # More chunks = more context but potentially less focused\n",
    "            \"max_chunks\": 5,\n",
    "            # Template for formatting each retrieved chunk\n",
    "            # {index}: Chunk number, {chunk.content}: Text content, {metadata}: Document metadata\n",
    "            \"chunk_template\": \"Result {index}\\nContent: {chunk.content}\\nMetadata: {metadata}\\n\",\n",
    "            # Search mode: \"vector\" uses semantic similarity search\n",
    "            # Alternative: \"keyword\" for keyword-based search\n",
    "            \"mode\": \"vector\"\n",
    "        },\n",
    "    )\n",
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b895a2-a00d-459c-986f-a7b6ba9d7167",
   "metadata": {},
   "source": [
    "#### Step 2: Complete RAG Pipeline Function\n",
    "\n",
    "This function combines retrieval and generation:\n",
    "1. **Retrieve**: Get relevant chunks from vector DB\n",
    "2. **Format**: Combine chunks into context string\n",
    "3. **Augment**: Add context to prompt\n",
    "4. **Generate**: Call LLM with augmented prompt\n",
    "5. **Return**: Final answer text\n",
    "\n",
    "**Prompt Engineering**:\n",
    "- Instructs LLM to only use provided context\n",
    "- Handles cases where answer isn't in context\n",
    "- Clear separation between question and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f2025-751f-4dbb-b6d9-8de470c889f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: Retrieve relevant chunks and generate answer.\n",
    "    \n",
    "    Args:\n",
    "        question: User's question to answer\n",
    "        \n",
    "    Returns:\n",
    "        Final answer text generated by LLM based on retrieved context\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant chunks via RAG tool\n",
    "    # This performs semantic search in the vector database\n",
    "    response = client.tool_runtime.rag_tool.query(\n",
    "        vector_db_ids=[vector_db_id],\n",
    "        content=question,\n",
    "        query_config={\n",
    "            \"query_generator_config\": {\n",
    "                \"type\": \"default\",\n",
    "                \"separator\": \" \"\n",
    "            },\n",
    "            \"max_tokens_in_context\": 4096,\n",
    "            \"max_chunks\": 5,\n",
    "            \"chunk_template\": \"Result {index}\\nContent: {chunk.content}\\nMetadata: {metadata}\\n\",\n",
    "            \"mode\": \"vector\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # 2. Extract plain text from retrieved chunks\n",
    "    #    (rag_res.content is a list of content items; each item has .text)\n",
    "    rag_text_chunks = []\n",
    "    for item in response.content:\n",
    "        # Depending on the client version, this may be item.text or item[\"text\"]\n",
    "        rag_text_chunks.append(str(item.text))\n",
    "\n",
    "    context = \"\\n\\n\".join(rag_text_chunks)\n",
    "\n",
    "    # 3. Build a prompt that includes both the question and the retrieved context\n",
    "    prompt = f\"\"\"You are a question-answering assistant.\n",
    "Answer the question ONLY using the context provided. \n",
    "If the answer is not in the context, respond with 'I don't know'.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    "\n",
    "    # 4. Ask the LLM to generate an answer using that context\n",
    "    completion = client.inference.chat_completion(\n",
    "        model_id=model_id,   # use your registered model id here\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "\n",
    "    # 5. Return the answer text\n",
    "    return completion.completion_message.content\n",
    "\n",
    "\n",
    "# Test the RAG pipeline\n",
    "answer = rag_pipeline(question=query)\n",
    "rich.print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250db81-37c3-4b39-bd49-f53e6e0fcded",
   "metadata": {},
   "source": [
    "### b. Using File Search API\n",
    "\n",
    "**Approach**: Simplified RAG via LlamaStack's Responses API.\n",
    "\n",
    "**Process**:\n",
    "1. Single API call handles retrieval + generation\n",
    "2. LlamaStack manages chunking, retrieval, and prompt construction\n",
    "3. Returns final answer directly\n",
    "\n",
    "**Advantages**:\n",
    "- Simpler code (one API call)\n",
    "- Less configuration needed\n",
    "- Built-in optimizations\n",
    "\n",
    "**Use Cases**: When you want a quick, production-ready RAG solution without fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e9a9e-21c9-4424-81d0-26acc554cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same query as before\n",
    "query = \"As per the documents, tell me about Percentage of shares held by Government of India in Indian bank \"\n",
    "\n",
    "# Use LlamaStack's Responses API for simplified RAG\n",
    "# This API handles retrieval + generation in a single call\n",
    "response = client.responses.create(\n",
    "    model=model_id,  # LLM model to use for generation\n",
    "    input=query,  # User's question\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"file_search\",  # Built-in RAG tool type\n",
    "            # vector_store_ids: Which vector databases to search\n",
    "            # The API will automatically:\n",
    "            # 1. Retrieve relevant chunks\n",
    "            # 2. Format them as context\n",
    "            # 3. Generate answer using LLM\n",
    "            \"vector_store_ids\": [vector_db_id],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "# Extract the output text from the response\n",
    "print(\"Responses API result:\", getattr(response, \"output_text\", response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a5093a-8b7f-46d3-9dbd-73458242b2ec",
   "metadata": {},
   "source": [
    "### c. Using RAG Agent\n",
    "\n",
    "**Approach**: Agent-based RAG with built-in knowledge search tool.\n",
    "\n",
    "**Process**:\n",
    "1. Create an Agent with `builtin::rag/knowledge_search` tool\n",
    "2. Agent automatically decides when to search documents\n",
    "3. Agent reasons about the query and generates answer\n",
    "\n",
    "**Advantages**:\n",
    "- Agent can reason about when to use RAG\n",
    "- Can combine with other tools\n",
    "- More flexible and extensible\n",
    "\n",
    "**Use Cases**: When building complex systems that need multiple tools and reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138be521-7be7-43da-bee8-654d7a4413f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User query\n",
    "query = \"As per the documents, tell me about Percentage of shares held by Government of India in Indian bank\"\n",
    "\n",
    "def agent_qa(user_question: str) -> str:\n",
    "    \"\"\"\n",
    "    RAG using Agent with built-in knowledge search tool.\n",
    "    \n",
    "    The Agent automatically:\n",
    "    - Decides when to search documents\n",
    "    - Retrieves relevant chunks\n",
    "    - Generates answer based on retrieved context\n",
    "    \n",
    "    Args:\n",
    "        user_question: User's question to answer\n",
    "        \n",
    "    Returns:\n",
    "        Final answer from the agent\n",
    "    \"\"\"\n",
    "    # Create an Agent with RAG knowledge search capability\n",
    "    agent = Agent(\n",
    "        client,  # LlamaStack client\n",
    "        model=model_id,  # LLM model for the agent\n",
    "        # Instructions guide the agent's behavior\n",
    "        # \"Answer strictly based on retrieved documents\" prevents hallucination\n",
    "        instructions=\"You are a helpful assistant. Answer strictly based on retrieved documents.\",\n",
    "        tools=[\n",
    "            {\n",
    "                # builtin::rag/knowledge_search: Built-in RAG tool\n",
    "                # Automatically searches vector databases and retrieves relevant chunks\n",
    "                \"name\": \"builtin::rag/knowledge_search\",\n",
    "                \"args\": {\n",
    "                    # vector_db_ids: Which vector databases to search\n",
    "                    \"vector_db_ids\": [vector_db_id]\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # Create a session for this conversation\n",
    "    # Sessions maintain conversation history and context\n",
    "    session_id = agent.create_session(\"web-session\")\n",
    "    \n",
    "    # Create a turn (one interaction) in the conversation\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",  # User message\n",
    "                \"content\": user_question,  # The question\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,  # Associate with this session\n",
    "        stream=False,  # Get complete response (not streaming)\n",
    "    )\n",
    "\n",
    "    # Extract the raw content from agent's response\n",
    "    raw_content = response.output_message.content\n",
    "\n",
    "    # Parse the response (may be JSON format)\n",
    "    # ReAct agents often return structured JSON with thought/action/answer\n",
    "    try:\n",
    "        react_obj = json.loads(raw_content)\n",
    "        # Extract the final answer from the structured response\n",
    "        final_answer = react_obj.get(\"answer\", raw_content)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: if not JSON, return raw content\n",
    "        final_answer = raw_content\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "# Test the agent-based RAG\n",
    "answer = agent_qa(user_question=query)\n",
    "rich.print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d270cfd-4515-4067-90b0-32fa37a9c0e7",
   "metadata": {},
   "source": [
    "### Limitations of Pure RAG\n",
    "\n",
    "**Problem**: RAG only searches ingested documents. It cannot answer questions about:\n",
    "- Real-time information (current stock prices, latest news)\n",
    "- Information not in the document corpus\n",
    "- Dynamic data that changes frequently\n",
    "\n",
    "**Example**: Asking about \"latest stock price\" when documents only contain historical financial data.\n",
    "\n",
    "**Solution**: Combine RAG with other tools (web search, APIs) using Agentic AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3ab8d-f81b-4a24-adf2-10dd034489c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query that RAG cannot answer (requires real-time data)\n",
    "query = \"can you tell me about Indian bank's stock latest price?\"\n",
    "\n",
    "# This will fail or give incomplete answer because:\n",
    "# 1. Documents contain historical financial data, not real-time prices\n",
    "# 2. Stock prices change constantly and aren't in static documents\n",
    "# 3. RAG can only retrieve from ingested documents\n",
    "rag_pipeline(question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9733f1-ac01-4316-bf23-db3b12939ca6",
   "metadata": {},
   "source": [
    "#### Solution: Multi-Tool Agentic AI\n",
    "\n",
    "**Approach**: Use agents that can select and use multiple tools:\n",
    "1. **RAG Tool**: For questions about ingested documents\n",
    "2. **Web Search Tool**: For real-time information and current events\n",
    "3. **Custom Tools**: For domain-specific data (e.g., stock prices via APIs)\n",
    "\n",
    "**Agent Capabilities**:\n",
    "- **Tool Selection**: Automatically chooses the right tool(s) for each query\n",
    "- **Multi-Step Reasoning**: Can use multiple tools in sequence\n",
    "- **Context Awareness**: Understands when to use RAG vs. web search vs. custom tools\n",
    "\n",
    "This creates a comprehensive system that handles both document-based and real-time queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b01ad26-ec92-4544-8351-9d645c748b99",
   "metadata": {},
   "source": [
    "# Part B: Using Web Search Tool for Real-Time Information\n",
    "\n",
    "## Web Search Integration\n",
    "\n",
    "### Why Web Search?\n",
    "- **Real-Time Data**: Access current information not in documents\n",
    "- **Broader Knowledge**: Search the entire web, not just ingested documents\n",
    "- **Dynamic Updates**: Information updates automatically\n",
    "\n",
    "### Implementation\n",
    "We'll use LlamaStack's built-in web search tool (`builtin::websearch`) which:\n",
    "- Searches the web using search APIs (e.g., Tavily)\n",
    "- Returns relevant web pages and snippets\n",
    "- Integrates seamlessly with agents\n",
    "\n",
    "### Setup Requirements\n",
    "- **API Key**: Tavily search API key (set as environment variable)\n",
    "- **Provider Data**: Pass API key to LlamaStackClient for web search access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0fb13e-0284-4830-abd3-5da6dca18bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Tavily search API key from environment variable\n",
    "# Tavily is a search API that provides web search capabilities\n",
    "tavily_search_api_key = os.getenv('TAVILY_SEARCH_API_KEY')\n",
    "\n",
    "# Configure provider data for web search\n",
    "# If API key is available, pass it to enable web search functionality\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None  # Web search will not be available\n",
    "else:\n",
    "    # provider_data: Configuration for external service providers\n",
    "    # tavily_search_api_key: API key for Tavily search service\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "# Reinitialize client with provider data for web search\n",
    "# provider_data enables the client to use external services like Tavily\n",
    "client = LlamaStackClient(\n",
    "    base_url=LLAMASTACK_URL,\n",
    "    provider_data=provider_data  # Enables web search if API key is provided\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0f7a2-22cb-4c2b-9960-a8e973f6e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that tavily search is available as a tool in Llamastack\n",
    "client.toolgroups.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66391da2-a141-4356-b7c6-1f6dfc8740e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.vector_dbs.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c46d4-6120-4033-9603-c1f98bfce230",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent(\n",
    "            client=client,\n",
    "            model=model_id,\n",
    "            tools=[\"builtin::websearch\"],\n",
    "            instructions=\"You are a helpful assistant. Answer strictly based on web search results.\",\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": ReActOutput.model_json_schema(),\n",
    "            },\n",
    "            sampling_params={'temperature':0.2, 'max_tokens':4000},\n",
    "        )\n",
    "\n",
    "qu= [\"Can you tell me the latest news about HDFC Bank?\"]\n",
    "stream=False\n",
    "\n",
    "session_id = agent.create_session(\"web-session\")\n",
    "for prompt in qu:\n",
    "    rich.print(f\"Processing user query: {prompt}\")\n",
    "    turn = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    raw_content = turn.output_message.content\n",
    "    data_dict = json.loads(raw_content)\n",
    "    answer = data_dict[\"answer\"]\n",
    "    \n",
    "\n",
    "    rich.print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
